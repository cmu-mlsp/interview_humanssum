{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read Data \n",
    "output_df = pd.read_csv('../data/release_data_summaries_nonexpert.tsv',sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "def get_pvalues(v1,v2):\n",
    "    res = ttest_ind(v1, v2)\n",
    "    return res.pvalue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Summary Lengths \n",
    "\n",
    "text_summary1_lengths = output_df[\"Text_Summary1\"].apply(lambda x: len(x.split(\" \")))\n",
    "text_summary2_lengths = output_df[\"Text_Summary2\"].apply(lambda x: len(x.split(\" \")))\n",
    "audio_summary1_lengths = output_df[\"Audio_Summary1\"].apply(lambda x: len(x.split(\" \")))\n",
    "audio_summary2_lengths = output_df[\"Audio_Summary2\"].apply(lambda x: len(x.split(\" \")))\n",
    "transcript_lengths = output_df[\"transcript\"].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "mean_text_summary = 0.5*(text_summary1_lengths + text_summary2_lengths)\n",
    "mean_audio_summary = 0.5*(audio_summary1_lengths+audio_summary2_lengths)\n",
    "\n",
    "audio_compression_ratio = [2*transcript_lengths[i]/(audio_summary1_lengths[i]+audio_summary2_lengths[i]) for i in range(len(audio_summary1_lengths))]\n",
    "text_compression_ratio = [2*transcript_lengths[i]/(text_summary1_lengths[i]+text_summary2_lengths[i]) for i in range(len(text_summary1_lengths))]\n",
    "\n",
    "\n",
    "print(f\"SLENGTH {mean_text_summary.mean()} {mean_text_summary.std()} {mean_audio_summary.mean()} {mean_audio_summary.std()} {get_pvalues(mean_text_summary,mean_audio_summary)}  \")\n",
    "print(f\"CRATIO {np.mean(audio_compression_ratio)} {np.std(audio_compression_ratio)} {np.mean(text_compression_ratio)} {np.std(text_compression_ratio)} {get_pvalues(audio_compression_ratio,text_compression_ratio)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2)\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.hist(text_summary1_lengths,bins=20)\n",
    "plt.title(\"Text 1 Summary Length\")\n",
    "plt.subplot(2,2,2)\n",
    "plt.hist(text_summary2_lengths,bins=20)\n",
    "plt.title(\"Text 2 Summary Length\")\n",
    "plt.subplot(2,2,3)\n",
    "plt.hist(audio_summary1_lengths,bins=20)\n",
    "plt.title(\"Audio 1 Summary Length\")\n",
    "plt.subplot(2,2,4)\n",
    "plt.hist(audio_summary2_lengths,bins=20)\n",
    "plt.title(\"Audio 2 Summary Length\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2)\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(text_summary1_lengths.tolist()+text_summary2_lengths.tolist(),bins=20)\n",
    "plt.title(\"Text Summary Length\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(audio_summary1_lengths.tolist()+audio_summary2_lengths.tolist(),bins=20)\n",
    "plt.title(\"Audio Summary Length\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unique Vocab Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    " \n",
    "# Load spaCy English model\n",
    "nlp = spacy.blank(\"en\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unique Vocab \n",
    "from itertools import chain\n",
    "\n",
    "text_summary1_unique = list(set(list(chain(*output_df[\"Text_Summary1\"].apply(lambda x: x.lower().split(\" \"))))))\n",
    "text_summary2_unique = list(set(list(chain(*output_df[\"Text_Summary2\"].apply(lambda x: x.lower().split(\" \"))))))\n",
    "audio_summary1_unique = list(set(list(chain(*output_df[\"Audio_Summary1\"].apply(lambda x: x.lower().split(\" \"))))))\n",
    "audio_summary2_unique = list(set(list(chain(*output_df[\"Audio_Summary2\"].apply(lambda x: x.lower().split(\" \"))))))\n",
    "\n",
    "ref_unique = list(set(list(chain(*output_df[\"transcript\"].apply(lambda x: x.lower().split(\" \"))))))\n",
    "\n",
    "\n",
    "print(\"Unique Text Summary 1 Vocab: \",len(text_summary1_unique))\n",
    "print(\"Unique Text Summary 2 Vocab: \",len(text_summary2_unique))\n",
    "print(\"Unique Audio Summary 1 Vocab: \",len(audio_summary1_unique))\n",
    "print(\"Unique Audio Summary 2 Vocab: \",len(audio_summary2_unique))\n",
    "\n",
    "print(\"Unique Text Summaries Vocab: \",len(set(text_summary1_unique+text_summary2_unique)))\n",
    "print(\"Unique Audio Summaries Vocab: \",len(set(audio_summary1_unique+audio_summary2_unique)))\n",
    "print(\"Unique Source Text Vocab\",len(set(ref_unique)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Novel Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Overlap with Transcript\n",
    "\n",
    "def filter_stopwords(text):\n",
    "    return [token.text for token in nlp(text) if not token.is_stop]\n",
    "\n",
    "novel_word_percent_text1 = output_df.apply(lambda x: len(set(filter_stopwords(x[\"Text_Summary1\"])).difference(set(filter_stopwords(x[\"transcript\"]))))/len(set(x[\"Text_Summary1\"].split(\" \"))),axis=1)\n",
    "novel_word_percent_text2 = output_df.apply(lambda x: len(set(filter_stopwords(x[\"Text_Summary2\"])).difference(set(filter_stopwords(x[\"transcript\"]))))/len(set(x[\"Text_Summary2\"].split(\" \"))),axis=1)\n",
    "novel_word_percent_audio1 = output_df.apply(lambda x: len(set(filter_stopwords(x[\"Audio_Summary1\"])).difference(set(filter_stopwords(x[\"transcript\"]))))/len(set(x[\"Audio_Summary1\"].split(\" \"))),axis=1)\n",
    "novel_word_percent_audio2 = output_df.apply(lambda x: len(set(filter_stopwords(x[\"Audio_Summary2\"])).difference(set(filter_stopwords(x[\"transcript\"]))))/len(set(x[\"Audio_Summary2\"].split(\" \"))),axis=1)\n",
    "\n",
    "print(f\"NOVWORD {0.5*(novel_word_percent_text1.mean()+novel_word_percent_text2.mean())} {np.std(novel_word_percent_text1.tolist()+novel_word_percent_text2.tolist())} {0.5*(novel_word_percent_audio1.mean()+novel_word_percent_audio2.mean())} {np.std(novel_word_percent_audio1.tolist()+novel_word_percent_audio2.tolist())} {get_pvalues(0.5*(novel_word_percent_text1+novel_word_percent_text2),0.5*(novel_word_percent_audio1+novel_word_percent_audio2))}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    " \n",
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "\n",
    "def get_entities(text):\n",
    "    return [ent.text for ent in nlp(text).ents]\n",
    "\n",
    "output_df[\"Text_Summary1_Entities\"] = output_df[\"Text_Summary1\"].apply(lambda x: get_entities(x))\n",
    "output_df[\"Text_Summary2_Entities\"] = output_df[\"Text_Summary2\"].apply(lambda x: get_entities(x))\n",
    "output_df[\"Audio_Summary1_Entities\"] = output_df[\"Audio_Summary1\"].apply(lambda x: get_entities(x))\n",
    "output_df[\"Audio_Summary2_Entities\"] = output_df[\"Audio_Summary2\"].apply(lambda x: get_entities(x))\n",
    "output_df[\"Transcript_Entities\"] = output_df[\"transcript\"].apply(lambda x: get_entities(x))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_entity_f1(ref_list,hyp_list):\n",
    "    ref_list = [x.lower() for x in ref_list]\n",
    "    hyp_list = [x.lower() for x in hyp_list]\n",
    "    ref_set = set(ref_list)\n",
    "    hyp_set = set(hyp_list)\n",
    "    tp = len(ref_set.intersection(hyp_set))\n",
    "    precision = tp/len(hyp_set) if len(hyp_set) > 0 else 0\n",
    "    recall = tp/len(ref_set) if len(ref_set) > 0 else 0\n",
    "    f1 = 2*precision*recall/(precision+recall) if precision+recall > 0 else 0\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "text_summary1_entity_f1 = output_df.apply(lambda x: process_entity_f1(x[\"Transcript_Entities\"],x[\"Text_Summary1_Entities\"]),axis=1)\n",
    "text_summary2_entity_f1 = output_df.apply(lambda x: process_entity_f1(x[\"Transcript_Entities\"],x[\"Text_Summary2_Entities\"]),axis=1)\n",
    "audio_summary1_entity_f1 = output_df.apply(lambda x: process_entity_f1(x[\"Transcript_Entities\"],x[\"Audio_Summary1_Entities\"]),axis=1)\n",
    "audio_summary2_entity_f1 = output_df.apply(lambda x: process_entity_f1(x[\"Transcript_Entities\"],x[\"Audio_Summary2_Entities\"]),axis=1)\n",
    "\n",
    "print(f\"ENTITYF1 {(text_summary1_entity_f1+text_summary2_entity_f1).mean()} {np.std(text_summary1_entity_f1.tolist()+text_summary2_entity_f1).tolist()} {(audio_summary1_entity_f1+audio_summary2_entity_f1).mean()} {np.std(audio_summary1_entity_f1.tolist()+audio_summary2_entity_f1).tolist()} {get_pvalues(text_summary1_entity_f1+text_summary2_entity_f1,audio_summary1_entity_f1+audio_summary2_entity_f1)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interannotator Agreement \n",
    "\n",
    "\n",
    "text_summary1_entity_f1_int = output_df.apply(lambda x: process_entity_f1(x[\"Text_Summary2_Entities\"],x[\"Text_Summary1_Entities\"]),axis=1)\n",
    "text_summary2_entity_f1_int = output_df.apply(lambda x: process_entity_f1(x[\"Text_Summary1_Entities\"],x[\"Text_Summary2_Entities\"]),axis=1)\n",
    "audio_summary1_entity_f1_int = output_df.apply(lambda x: process_entity_f1(x[\"Audio_Summary2_Entities\"],x[\"Audio_Summary1_Entities\"]),axis=1)\n",
    "audio_summary2_entity_f1_int = output_df.apply(lambda x: process_entity_f1(x[\"Audio_Summary1_Entities\"],x[\"Audio_Summary2_Entities\"]),axis=1)\n",
    "\n",
    "print(f\"INTENTF1 {(text_summary1_entity_f1_int+text_summary2_entity_f1_int).mean()} {np.std(text_summary1_entity_f1.tolist()+text_summary2_entity_f1).tolist()} {(audio_summary1_entity_f1_int+audio_summary2_entity_f1_int).mean()} {np.std(audio_summary1_entity_f1.tolist()+audio_summary2_entity_f1).tolist()} {get_pvalues(text_summary1_entity_f1_int+text_summary2_entity_f1_int,audio_summary1_entity_f1_int+audio_summary2_entity_f1_int)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics with Transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGE-L with Transcript "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate \n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "def compute_rouge(hyp,ref):\n",
    "    results = rouge.compute(predictions=[hyp],references=[ref])\n",
    "    return 100*results[\"rougeL\"]\n",
    "\n",
    "text_summary1_transcript_rouge = output_df.apply(lambda x: compute_rouge(x[\"Text_Summary1\"],x[\"transcript\"]),axis=1).tolist()\n",
    "text_summary2_transcript_rouge = output_df.apply(lambda x: compute_rouge(x[\"Text_Summary2\"],x[\"transcript\"]),axis=1).tolist()\n",
    "audio_summary1_transcript_rouge = output_df.apply(lambda x: compute_rouge(x[\"Audio_Summary1\"],x[\"transcript\"]),axis=1).tolist()\n",
    "audio_summary2_transcript_rouge = output_df.apply(lambda x: compute_rouge(x[\"Audio_Summary2\"],x[\"transcript\"]),axis=1).tolist()\n",
    "\n",
    "print(f\"RL {np.mean(text_summary1_transcript_rouge+text_summary2_transcript_rouge)} {np.std(text_summary1_transcript_rouge+text_summary2_transcript_rouge)} {np.mean(audio_summary1_transcript_rouge+audio_summary2_transcript_rouge)} {np.std(audio_summary1_transcript_rouge+audio_summary2_transcript_rouge)} {get_pvalues(text_summary1_transcript_rouge+text_summary2_transcript_rouge,audio_summary1_transcript_rouge+audio_summary2_transcript_rouge)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate \n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "def compute_rouge(hyp,ref):\n",
    "    results = rouge.compute(predictions=[hyp],references=[ref])\n",
    "    return 100*results[\"rougeL\"]\n",
    "\n",
    "text_summary1_transcript_rouge = output_df.apply(lambda x: compute_rouge(x[\"Text_Summary1\"],x[\"Text_Summary2\"]),axis=1).tolist()\n",
    "text_summary2_transcript_rouge = output_df.apply(lambda x: compute_rouge(x[\"Text_Summary2\"],x[\"Text_Summary1\"]),axis=1).tolist()\n",
    "audio_summary1_transcript_rouge = output_df.apply(lambda x: compute_rouge(x[\"Audio_Summary1\"],x[\"Audio_Summary2\"]),axis=1).tolist()\n",
    "audio_summary2_transcript_rouge = output_df.apply(lambda x: compute_rouge(x[\"Audio_Summary2\"],x[\"Audio_Summary1\"]),axis=1).tolist()\n",
    "\n",
    "\n",
    "print(f\"RLINT {np.mean(text_summary1_transcript_rouge+text_summary2_transcript_rouge)} {np.std(text_summary1_transcript_rouge+text_summary2_transcript_rouge)} {np.mean(audio_summary1_transcript_rouge+audio_summary2_transcript_rouge)} {np.std(audio_summary1_transcript_rouge+audio_summary2_transcript_rouge)} {get_pvalues(text_summary1_transcript_rouge+text_summary2_transcript_rouge,audio_summary1_transcript_rouge+audio_summary2_transcript_rouge)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## METEOR with Transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate \n",
    "\n",
    "\n",
    "meteor = evaluate.load('meteor')\n",
    "\n",
    "def compute_meteor(hyp,ref):\n",
    "    results = meteor.compute(predictions=[hyp.lower()],references=[ref.lower()])\n",
    "    return 100*results[\"meteor\"]\n",
    "\n",
    "text_summary1_transcript_meteor = output_df.apply(lambda x: compute_meteor(x[\"Text_Summary1\"],x[\"transcript\"]),axis=1).tolist()\n",
    "text_summary2_transcript_meteor = output_df.apply(lambda x: compute_meteor(x[\"Text_Summary2\"],x[\"transcript\"]),axis=1).tolist()\n",
    "audio_summary1_transcript_meteor = output_df.apply(lambda x: compute_meteor(x[\"Audio_Summary1\"],x[\"transcript\"]),axis=1).tolist()\n",
    "audio_summary2_transcript_meteor = output_df.apply(lambda x: compute_meteor(x[\"Audio_Summary2\"],x[\"transcript\"]),axis=1).tolist()\n",
    "\n",
    "print(f\"MTR {np.mean(text_summary1_transcript_meteor+text_summary2_transcript_meteor)} {np.std(text_summary1_transcript_meteor+text_summary2_transcript_meteor)} {np.mean(audio_summary1_transcript_meteor+audio_summary2_transcript_meteor)} {np.std(audio_summary1_transcript_meteor+audio_summary2_transcript_meteor)} {get_pvalues(text_summary1_transcript_meteor+text_summary2_transcript_meteor,audio_summary1_transcript_meteor+audio_summary2_transcript_meteor)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTScore with Transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "bertscore = load(\"bertscore\")\n",
    "\n",
    "def compute_bertscore(hyp,ref):\n",
    "    results = bertscore.compute(predictions=[hyp.lower()],references=[ref.lower()],lang=\"en\")\n",
    "    return 100*np.mean(results[\"f1\"])\n",
    "\n",
    "text_summary1_transcript_bertscore = output_df.apply(lambda x: compute_bertscore(x[\"Text_Summary1\"],x[\"transcript\"]),axis=1).tolist()\n",
    "text_summary2_transcript_bertscore = output_df.apply(lambda x: compute_bertscore(x[\"Text_Summary2\"],x[\"transcript\"]),axis=1).tolist()\n",
    "audio_summary1_transcript_bertscore = output_df.apply(lambda x: compute_bertscore(x[\"Audio_Summary1\"],x[\"transcript\"]),axis=1).tolist()\n",
    "audio_summary2_transcript_bertscore = output_df.apply(lambda x: compute_bertscore(x[\"Audio_Summary2\"],x[\"transcript\"]),axis=1).tolist()\n",
    "\n",
    "print(f\"BS {np.mean(text_summary1_transcript_bertscore+text_summary2_transcript_bertscore)} {np.std(text_summary1_transcript_bertscore+text_summary2_transcript_bertscore)} {np.mean(audio_summary1_transcript_bertscore+audio_summary2_transcript_bertscore)} {np.std(audio_summary1_transcript_bertscore+audio_summary2_transcript_bertscore)} {get_pvalues(text_summary1_transcript_bertscore+text_summary2_transcript_bertscore,audio_summary1_transcript_bertscore+audio_summary2_transcript_bertscore)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_summary1_transcript_bertscore = output_df.apply(lambda x: compute_bertscore(x[\"Text_Summary1\"],x[\"Text_Summary2\"]),axis=1).tolist()\n",
    "text_summary2_transcript_bertscore = output_df.apply(lambda x: compute_bertscore(x[\"Text_Summary2\"],x[\"Text_Summary1\"]),axis=1).tolist()\n",
    "audio_summary1_transcript_bertscore = output_df.apply(lambda x: compute_bertscore(x[\"Audio_Summary1\"],x[\"Audio_Summary2\"]),axis=1).tolist()\n",
    "audio_summary2_transcript_bertscore = output_df.apply(lambda x: compute_bertscore(x[\"Audio_Summary2\"],x[\"Audio_Summary1\"]),axis=1).tolist()\n",
    "\n",
    "\n",
    "print(f\"BSINT {np.mean(text_summary1_transcript_bertscore+text_summary2_transcript_bertscore)} {np.std(text_summary1_transcript_bertscore+text_summary2_transcript_bertscore)} {np.mean(audio_summary1_transcript_bertscore+audio_summary2_transcript_bertscore)} {np.std(audio_summary1_transcript_bertscore+audio_summary2_transcript_bertscore)} {get_pvalues(text_summary1_transcript_bertscore+text_summary2_transcript_bertscore,audio_summary1_transcript_bertscore+audio_summary2_transcript_bertscore)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_summary1 = list(output_df[\"Text_Summary1\"])\n",
    "text_summary2 = list(output_df[\"Text_Summary2\"])\n",
    "audio_summary1 = list(output_df[\"Audio_Summary1\"])\n",
    "audio_summary2 = list(output_df[\"Audio_Summary2\"])\n",
    "src = list(output_df[\"transcript\"])\n",
    "ref = list(output_df[\"ref\"])\n",
    "\n",
    "\n",
    "print(len(text_summary1),len(text_summary2),len(audio_summary1),len(audio_summary2),len(src),len(ref))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UniEval with Transcript "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%cd UniEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import convert_to_json\n",
    "from metric.evaluator import get_evaluator\n",
    "\n",
    "task = 'summarization'\n",
    "\n",
    "evaluator = get_evaluator(task)\n",
    "\n",
    "\n",
    "def compute_unieval_scores(hyp_list,ref_list,src_list):\n",
    "    data = convert_to_json(src_list=src_list, ref_list=ref_list,output_list=hyp_list)\n",
    "    eval_scores = evaluator.evaluate(data, dims=['coherence', 'consistency', 'fluency'], \n",
    "                                 overall=False, print_result=False)\n",
    "    return [x[\"coherence\"] for x in eval_scores],[x[\"consistency\"] for x in eval_scores],[x[\"fluency\"] for x in eval_scores]\n",
    "\n",
    "\n",
    "\n",
    "ts1_coh,ts1_con,ts1_flu = compute_unieval_scores(text_summary1,ref,src)\n",
    "ts2_coh,ts2_con,ts2_flu = compute_unieval_scores(text_summary2,ref,src)\n",
    "ss1_coh,ss1_con,ss1_flu = compute_unieval_scores(audio_summary1,ref,src)\n",
    "ss2_coh,ss2_con,ss2_flu = compute_unieval_scores(audio_summary2,ref,src)\n",
    "\n",
    "print(f\"COH {np.mean(ts1_coh+ts2_coh)} {np.std(ts1_coh+ts2_coh)} {np.mean(ss1_coh+ss2_coh)} {np.std(ss1_coh+ss2_coh)} {get_pvalues(ts1_coh+ts2_coh,ss1_coh+ss2_coh)}\")\n",
    "print(f\"CON {np.mean(ts1_con+ts2_con)} {np.std(ts1_con+ts2_con)} {np.mean(ss1_con+ss2_con)} {np.std(ss1_con+ss2_con)} {get_pvalues(ts1_con+ts2_con,ss1_con+ss2_con)}\")\n",
    "print(f\"FL {np.mean(ts1_flu+ts2_flu)} {np.std(ts1_flu+ts2_flu)} {np.mean(ss1_flu+ss2_flu)} {np.std(ss1_flu+ss2_flu)} {get_pvalues(ts1_flu+ts2_flu,ss1_flu+ss2_flu)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interannotator agreement \n",
    "\n",
    "task = 'summarization'\n",
    "evaluator = get_evaluator(task)\n",
    "\n",
    "\n",
    "def compute_unieval_scores(hyp_list,ref_list,src_list):\n",
    "    data = convert_to_json(src_list=src_list, ref_list=ref_list,output_list=hyp_list)\n",
    "    eval_scores = evaluator.evaluate(data, dims=['relevance'], \n",
    "                                 overall=False, print_result=False)\n",
    "    return [x[\"relevance\"] for x in eval_scores]\n",
    "\n",
    "\n",
    "\n",
    "ts1_rel = compute_unieval_scores(text_summary1,text_summary2,src)\n",
    "ts2_rel = compute_unieval_scores(text_summary2,text_summary1,src)\n",
    "ss1_rel = compute_unieval_scores(audio_summary1,audio_summary2,src)\n",
    "ss2_rel = compute_unieval_scores(audio_summary2,audio_summary1,src)\n",
    "\n",
    "print(f\"RELINT {np.mean(ts1_rel+ts2_rel)} {np.std(ts1_rel+ts2_rel)} {np.mean(ss1_rel+ss2_rel)} {np.std(ss1_rel+ss2_rel)} {get_pvalues(ts1_rel+ts2_rel,ss1_rel+ss2_rel)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Factual Consistency between Modality \n",
    "task = 'summarization'\n",
    "evaluator = get_evaluator(task)\n",
    "\n",
    "\n",
    "def compute_unieval_scores(hyp_list,ref_list,src_list):\n",
    "    data = convert_to_json(src_list=src_list, ref_list=ref_list,output_list=hyp_list)\n",
    "    eval_scores = evaluator.evaluate(data, dims=['relevance'], \n",
    "                                 overall=False, print_result=True)\n",
    "    return [x[\"relevance\"] for x in eval_scores]\n",
    "\n",
    "\n",
    "\n",
    "ts12_rel =  compute_unieval_scores(text_summary1,audio_summary2,src)\n",
    "ts11_rel =  compute_unieval_scores(text_summary1,audio_summary1,src)\n",
    "\n",
    "ts22_rel =  compute_unieval_scores(text_summary2,audio_summary2,src)\n",
    "ts21_rel = compute_unieval_scores(text_summary2,audio_summary1,src)\n",
    "\n",
    "ss12_rel =  compute_unieval_scores(audio_summary1,text_summary2,src)\n",
    "ss11_rel = compute_unieval_scores(audio_summary1,text_summary1,src)\n",
    "\n",
    "ss22_rel = compute_unieval_scores(audio_summary2,text_summary2,src)\n",
    "ss21_rel = compute_unieval_scores(audio_summary2,text_summary1,src)\n",
    "\n",
    "text_all = ts12_rel + ts11_rel + ts21_rel + ts22_rel\n",
    "audio_all = ss12_rel + ss11_rel + ss21_rel + ss22_rel\n",
    "\n",
    "\n",
    "\n",
    "print(f\"RELPAIR {np.mean(text_all)} {np.std(text_all)} {np.mean(audio_all)} {np.std(audio_all)} {get_pvalues(text_all,audio_all)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'summarization'\n",
    "\n",
    "evaluator = get_evaluator(task)\n",
    "\n",
    "\n",
    "def compute_unieval_scores(hyp_list,ref_list,src_list):\n",
    "    data = convert_to_json(src_list=src_list, ref_list=ref_list,output_list=hyp_list)\n",
    "    eval_scores = evaluator.evaluate(data, dims=['consistency'], \n",
    "                                 overall=False, print_result=True)\n",
    "    return [x[\"consistency\"] for x in eval_scores]\n",
    "\n",
    "\n",
    "\n",
    "ts12_rel =  compute_unieval_scores(text_summary1,audio_summary2,audio_summary2)\n",
    "ts11_rel =  compute_unieval_scores(text_summary1,audio_summary1,audio_summary1)\n",
    "\n",
    "ts22_rel =  compute_unieval_scores(text_summary2,audio_summary2,audio_summary2)\n",
    "ts21_rel = compute_unieval_scores(text_summary2,audio_summary1,audio_summary1)\n",
    "\n",
    "ss12_rel = compute_unieval_scores(audio_summary1,text_summary2,text_summary2)\n",
    "ss11_rel = compute_unieval_scores(audio_summary1,text_summary1,text_summary1)\n",
    "\n",
    "ss22_rel = compute_unieval_scores(audio_summary2,text_summary2,text_summary2)\n",
    "ss21_rel = compute_unieval_scores(audio_summary2,text_summary1,text_summary1)\n",
    "\n",
    "\n",
    "text_all = ts12_rel + ts11_rel + ts21_rel + ts22_rel\n",
    "audio_all = ss12_rel + ss11_rel + ss21_rel + ss22_rel\n",
    "\n",
    "\n",
    "print(f\"CONPAIR {np.mean(text_all)} {np.std(text_all)} {np.mean(audio_all)} {np.std(audio_all)} {get_pvalues(text_all,audio_all)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BARTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd BARTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_score import BARTScorer\n",
    "bart_scorer = BARTScorer(device='cuda:0', checkpoint='facebook/bart-large-cnn')\n",
    "\n",
    "text1_scores = bart_scorer.score(text_summary1,src,batch_size=4)\n",
    "text2_scores = bart_scorer.score(text_summary2,src,batch_size=4)\n",
    "audio1_scores = bart_scorer.score(audio_summary1,src,batch_size=4)\n",
    "audio2_scores = bart_scorer.score(audio_summary2,src,batch_size=4)\n",
    "\n",
    "text_score = np.mean(text1_scores+text2_scores)\n",
    "audio_score = np.mean(audio1_scores+audio2_scores) \n",
    "\n",
    "print(f\"BARTTrans {text_score} {np.std(text1_scores+text2_scores)} {audio_score} {np.std(audio1_scores+audio2_scores)} {get_pvalues(text1_scores+text2_scores,audio1_scores+audio2_scores)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1_scores = bart_scorer.score(text_summary1,ref,batch_size=4)\n",
    "text2_scores = bart_scorer.score(text_summary2,ref,batch_size=4)\n",
    "audio1_scores = bart_scorer.score(audio_summary1,ref,batch_size=4)\n",
    "audio2_scores = bart_scorer.score(audio_summary2,ref,batch_size=4)\n",
    "\n",
    "text_score = np.mean(text1_scores+text2_scores)\n",
    "audio_score = np.mean(audio1_scores+audio2_scores) \n",
    "\n",
    "print(f\"BARTRef {text_score} {np.std(text1_scores+text2_scores)} {audio_score} {np.std(audio1_scores+audio2_scores)} {get_pvalues(text1_scores+text2_scores,audio1_scores+audio2_scores)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_interannotator_bart_score(hyp,ref):\n",
    "    results1 = bart_scorer.score(hyp,ref,batch_size=4)\n",
    "    results2 = bart_scorer.score(ref,hyp,batch_size=4)\n",
    "    return np.mean(results1+results2),np.std(results1+results2),results1+results2\n",
    "\n",
    "text_interannotator_bartscore,text_std,text_sc = get_interannotator_bart_score(text_summary1,text_summary2)\n",
    "audio_interannotator_bartscore,audio_std,audio_sc = get_interannotator_bart_score(audio_summary1,audio_summary2)\n",
    "\n",
    "print(f\"BARTINT {text_interannotator_bartscore} {text_std} {audio_interannotator_bartscore} {audio_std} {get_pvalues(text_sc,audio_sc)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text1_ref_speech1hyp = bart_scorer.score(audio_summary1,text_summary1,batch_size=4)\n",
    "text1_ref_speech2hyp = bart_scorer.score(audio_summary2,text_summary1,batch_size=4)\n",
    "\n",
    "text2_ref_speech1hyp = bart_scorer.score(audio_summary1,text_summary2,batch_size=4)\n",
    "text2_ref_speech2hyp = bart_scorer.score(audio_summary2,text_summary2,batch_size=4)\n",
    "\n",
    "speech1_ref_text1hyp = bart_scorer.score(text_summary1,audio_summary1,batch_size=4)\n",
    "speech1_ref_text2hyp = bart_scorer.score(text_summary2,audio_summary1,batch_size=4)\n",
    "\n",
    "speech2_ref_text1hyp = bart_scorer.score(text_summary1,audio_summary2,batch_size=4)\n",
    "speech2_ref_text2hyp = bart_scorer.score(text_summary2,audio_summary2,batch_size=4)\n",
    "\n",
    "\n",
    "text_all = text1_ref_speech1hyp+text1_ref_speech2hyp+text2_ref_speech1hyp+text2_ref_speech2hyp\n",
    "speech_all = speech1_ref_text1hyp+speech1_ref_text2hyp+speech2_ref_text1hyp+speech2_ref_text2hyp\n",
    "\n",
    "print(\"Text Ref BS\",np.mean(text_all))\n",
    "print(\"Speech Ref BS\",np.mean(speech_all))\n",
    "\n",
    "print(\"Text Ref BS STD\",np.std(text_all))\n",
    "print(\"Speech Ref BS STD\",np.std(speech_all))\n",
    "\n",
    "print(f\"Stat Result {get_pvalues(text_all,speech_all)}\")\n",
    "\n",
    "\n",
    "print(f\"BARTPAIR {np.mean(text_all)} {np.std(text_all)} {np.mean(speech_all)} {np.std(speech_all)} {get_pvalues(text_all,speech_all)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval based Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import torchmetrics\n",
    "import torch\n",
    "\n",
    "def get_bertscore_retrieval(hyp_summary,refs):\n",
    "    hyp = [hyp_summary]*len(refs)\n",
    "    results = bertscore.compute(predictions=hyp,references=refs,lang=\"en\",device=\"cuda:0\")\n",
    "    return results[\"f1\"]\n",
    "\n",
    "    \n",
    "def get_retrieval_acc_entropy(hyp_summary,gt_index):\n",
    "    \n",
    "    scores = np.array(get_bertscore_retrieval(hyp_summary,ref))\n",
    "    index = np.argmax(scores,axis=0)\n",
    "    retrieval_acc = 1 if index == gt_index else 0\n",
    "    mrr = get_mrr(scores,gt_index)    \n",
    "    return retrieval_acc,mrr\n",
    "\n",
    "def get_mrr(scores,gt_index):\n",
    "    gt_index_array = torch.from_numpy(np.array([False]*len(scores),dtype=bool))\n",
    "    gt_index_array[gt_index] = True\n",
    "    scores = torch.from_numpy(scores)\n",
    "    mrr = torchmetrics.functional.retrieval.retrieval_reciprocal_rank(scores, gt_index_array, top_k=None)\n",
    "    \n",
    "    return mrr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_summary1_retr = output_df.apply(lambda x: get_retrieval_acc_entropy(x[\"Text_Summary1\"],x.name),axis=1)\n",
    "text_summary2_retr = output_df.apply(lambda x: get_retrieval_acc_entropy(x[\"Text_Summary2\"],x.name),axis=1)\n",
    "\n",
    "\n",
    "audio_summary1_retr = output_df.apply(lambda x: get_retrieval_acc_entropy(x[\"Audio_Summary1\"],x.name),axis=1)\n",
    "audio_summary2_retr = output_df.apply(lambda x: get_retrieval_acc_entropy(x[\"Audio_Summary2\"],x.name),axis=1)\n",
    "\n",
    "\n",
    "text_acc = [x[0] for x in text_summary1_retr] + [x[0] for x in text_summary2_retr]\n",
    "text_mrr = [x[1] for x in text_summary1_retr] + [x[1] for x in text_summary2_retr]\n",
    "\n",
    "audio_acc = [x[0] for x in audio_summary1_retr] + [x[0] for x in audio_summary2_retr]\n",
    "audio_mrr = [x[1] for x in audio_summary1_retr] + [x[1] for x in audio_summary2_retr]\n",
    "\n",
    "\n",
    "print(f\"RETRACC {np.mean(text_acc)} {np.std(text_acc)} {np.mean(audio_acc)} {np.std(audio_acc)} {get_pvalues(text_acc,audio_acc)}\")\n",
    "print(f\"RETRMRR {np.mean(text_mrr)} {np.std(text_mrr)} {np.mean(audio_mrr)} {np.std(audio_mrr)} {get_pvalues(text_acc,audio_mrr)}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
